# -*- coding: utf-8 -*-
"""deberta-ft.ipynb

Automatically generated by Colaboratory.

Python notebook to fine-tune a Deberta model on the FEVER dataset.
"""

# python3 -m venv .venv
# source .venv/bin/activate
# pip install transformers datasets evaluate sentencepiece accelerate sentence-transformers qdrant-client

from datasets import load_dataset

fever_wiki = load_dataset('fever', 'wiki_pages')

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('BAAI/bge-small-en-v1.5')

titles = fever_wiki['wikipedia_pages']['id']
title_embeddings = model.encode(titles)

fever_wiki['wikipedia_pages'][2]['lines'].split('\n')

fever_wiki['wikipedia_pages'].filter(lambda example: example["id"] == 'Roman_Atwood')[0]['lines'].split('\n')

from transformers import AutoTokenizer

fever_train = load_dataset('fever', 'v1.0', split='train')
fever_eval = load_dataset('fever', 'v1.0', split='labelled_dev')

tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')

label2id = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT ENOUGH INFO': 2}
id2label = {0: 'SUPPORTS', 1: 'REFUTES', 2: 'NOT ENOUGH INFO'}

def preprocess_function(examples):
    tokenized_inputs = tokenizer(examples['claim'], max_length=512, truncation=True)
    tokenized_inputs['label'] = [label2id[label] for label in examples['label']]
    return tokenized_inputs

tokenized_train = fever_train.map(preprocess_function, batched=True)
tokenized_eval = fever_eval.map(preprocess_function, batched=True)

tokenized_datasets = {'train': tokenized_train, 'labelled_dev': tokenized_eval}

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-large', num_labels=3, id2label=id2label, label2id=label2id)

from transformers import Trainer, TrainingArguments, DataCollatorWithPadding
import evaluate
import numpy as np

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

training_args = TrainingArguments(
    fp16=True,
    output_dir='./results',
    warmup_steps=500,
    logging_dir='./logs',
    logging_steps=10,
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['labelled_dev'],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.evaluate()

model.save_pretrained('./fine-tuned-deberta-v3-large-fever')

from transformers import pipeline

claim = "The Earth is round."
classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)
classifier(claim)
